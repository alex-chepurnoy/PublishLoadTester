# Test Matrix Implementation Plan

## Overview

This document outlines the phased approach to implementing the complete TEST_MATRIX functionality in the orchestration system. The plan is structured to allow incremental development with testing at each phase.

**Goal**: Implement adaptive load testing with 72 tests (3 protocols √ó 4 resolutions √ó 6 connection levels) with intelligent stopping based on server resource thresholds.

## System Architecture

```
ORCHESTRATOR/CLIENT (8 cores, 16GB RAM)    SERVER (4 cores, 8GB RAM)
‚îú‚îÄ Runs orchestrator script           ‚îÄ‚îÄ‚ñ∫ ‚îú‚îÄ Runs Wowza/streaming server
‚îú‚îÄ Encodes video streams                  ‚îú‚îÄ Processes incoming streams
‚îú‚îÄ Monitors server via SSH                ‚îú‚îÄ LIMITED RESOURCES (test target)
‚îî‚îÄ Generates reports                      ‚îî‚îÄ CPU/Heap monitoring critical
```

**Key Point**: The SERVER (4-core/8GB) is what we're testing. The orchestrator monitors it remotely via SSH.

---

## Current State Analysis

### ‚úÖ Already Implemented
- SSH connectivity and authentication
- Remote server monitoring (CPU via `get_server_cpu()`)
- Basic monitoring tools: `pidstat`, `sar`, `ifstat`
- Basic test execution loop
- Python dependency checking
- Result parsing and CSV generation
- Server CPU checking before tests
- Pilot mode for quick validation
- Graceful cleanup and interrupt handling
- Remote log fetching

### ‚ùå Needs Implementation (Per TEST_MATRIX)
- **Live heap memory monitoring** (critical for adaptive stopping)
- **Real-time threshold checking** (CPU AND Heap before each test)
- **Enhanced server-side logging** (comprehensive metrics capture)
- Single bitrate per resolution (currently uses 3 bitrates)
- Fixed codec selection (H.264 + AAC only)
- 6 connection levels: 1, 5, 10, 20, 50, **100** (currently: 1, 2, 5, 10, 20, 50)
- 15-minute test duration (currently: 10 minutes default)
- 30-second cooldown between tests (currently: 5-10 seconds)
- **Resolution-first order** (currently: protocol-first)
- **Adaptive stopping at 80% CPU OR 80% Heap**
- Skip remaining connection tests when threshold reached
- Maximum capacity logging
- Test matrix summary report

### üîç Questions to Address First
1. How to monitor server CPU/Heap **live** from orchestrator during tests?
2. How to ensure server captures and stores all metrics reliably?
3. What happens if monitoring tools fail or connection drops?
4. How to validate monitoring is working before starting tests?

---

## Implementation Phases

## ‚úÖ Phase 0: Monitoring Infrastructure & Validation [COMPLETE]
**Goal**: Establish robust server monitoring before any test changes

**Priority**: üî• CRITICAL - Must complete before other phases  
**Status**: ‚úÖ **COMPLETE** (Production validated on EC2 Wowza)  
**Time Spent**: ~12 hours (10 major fixes applied)  
**Complexity**: High (Multi-GC support, sudo permissions, AWK processing)

**üìÑ Summary Document**: [orchestrator/docs/phase0/PHASE_0_FINAL_SUMMARY.md](phase0/PHASE_0_FINAL_SUMMARY.md)

### Why Phase 0 Comes First

The adaptive stopping feature depends entirely on accurate, real-time server monitoring. Without this:
- Can't detect 80% thresholds
- Can't stop tests safely
- Can't log maximum capacity
- Risk overloading server

### Tasks

#### 0.1 Document Current Monitoring Capabilities ‚úÖ
- [x] Review existing `get_server_cpu()` function
- [x] Review existing `remote_start_monitors()` function
- [x] Review existing monitoring tools: `pidstat`, `sar`, `ifstat`, `jstat`
- [x] Identify gaps in current implementation

**Current Monitoring Status**:
```bash
‚úÖ CPU: get_server_cpu() - System CPU via mpstat/sar
‚úÖ Heap: get_server_heap() - Java heap via jcmd with multi-GC support (Parallel, G1, ZGC, Shenandoah)
‚úÖ Memory: get_server_memory() - RAM via free command
‚úÖ Network: get_server_network() - Mbps via sar/ifstat
‚úÖ Status: check_server_status() - Unified "CPU|HEAP|MEM|NET" check
‚úÖ Remote: remote_monitor.sh - 5-second CSV logging to monitors/
‚úÖ Validation: validate_server.sh - Pre-flight validation (6 checks)
‚úÖ Diagnostic: diagnose_jcmd.sh - Troubleshoot heap monitoring
```

#### 0.2 Implement Live Heap Monitoring Function ‚úÖ
- [x] Create `get_server_heap()` function
- [x] Detect Wowza PID reliably (Bootstrap Engine, not Manager)
- [x] Query Java heap using `jcmd` (primary method)
- [x] Fallback to `jstat` if jcmd unavailable
- [x] Fallback to `jmap` if both fail (emergency fallback)
- [x] Calculate heap percentage: `(used / max) * 100`
- [x] Handle missing PID gracefully (return 0.00)
- [x] Handle missing tools gracefully (return 0.00)
- [x] Add timeout protection (10 seconds max)
- [x] Log heap query results
- [x] **FIX**: Support multiple GC types (Parallel, G1GC, ZGC, Shenandoah)
- [x] **FIX**: Parse ZGC MB format (not KB)
- [x] **FIX**: Dual-location Java tool detection (PATH + Wowza bundled)
- [x] **FIX**: Passwordless sudo for root processes
- [x] **FIX**: Local AWK processing (not remote over SSH)

**File**: `orchestrator/run_orchestration.sh` (lines ~390-490)

**Why jcmd?**
- ‚úÖ More reliable than jstat
- ‚úÖ Easier to parse (human-readable output)
- ‚úÖ Part of standard JDK (same as jstat)
- ‚úÖ Better for scripting
- ‚úÖ Modern recommended approach
- ‚úÖ Supports all modern GC types

**Implementation Status**: ‚úÖ COMPLETE with multi-GC support
- Supports Parallel GC, G1GC, ZGC, Shenandoah
- 4-level tool cascade: PATH jcmd, Wowza jcmd, sudo PATH jcmd, sudo Wowza jcmd
- Processes data locally (gets output via SSH, parses with AWK locally)
- Returns heap percentage: 0.00-100.00
- Tested on production EC2 Wowza with ZGC (Generational mode)
```bash
function get_server_heap() {
  local heap_raw
  local wowza_pid
  
  # Get Wowza PID from running process
  wowza_pid=$(timeout 10 ssh -i "$KEY_PATH" $SSH_OPTS "$SSH_USER@$SERVER_IP" \
    "ps aux | grep -E '[Ww]owza|java.*com.wowza' | grep -v grep | head -n1 | awk '{print \$2}'" 2>/dev/null || echo "")
  
  if [[ -z "$wowza_pid" ]]; then
    log "WARNING: Could not detect Wowza PID for heap monitoring"
    echo "0.00"
    return
  fi
  
  log "Detected Wowza PID: $wowza_pid"
  
  # Primary method: Use jcmd GC.heap_info
  # This gives us clear "used" and "capacity" values
  heap_raw=$(timeout 10 ssh -i "$KEY_PATH" $SSH_OPTS "$SSH_USER@$SERVER_IP" \
    "jcmd $wowza_pid GC.heap_info 2>/dev/null | awk '
      /PSYoungGen/ { in_young=1 }
      /ParOldGen|PSOldGen/ { in_old=1; in_young=0 }
      /total/ && (in_young || in_old) {
        for(i=1; i<=NF; i++) {
          if(\$i ~ /^[0-9]+K/) {
            gsub(/K/, \"\", \$i)
            total_kb += \$i
          }
        }
      }
      /used/ && (in_young || in_old) {
        for(i=1; i<=NF; i++) {
          if(\$i ~ /^[0-9]+K/) {
            gsub(/K/, \"\", \$i)
            used_kb += \$i
          }
        }
      }
      END {
        if(total_kb > 0) {
          printf \"%.2f\", (used_kb / total_kb) * 100
        } else {
          print \"0.00\"
        }
      }
    '" 2>/dev/null || echo "")
  
  # Alternative jcmd method: Use GC.class_histogram (simpler, faster)
  if [[ -z "$heap_raw" ]] || [[ "$heap_raw" == "0.00" ]]; then
    log "Trying alternative jcmd method (VM.native_memory)..."
    heap_raw=$(timeout 10 ssh -i "$KEY_PATH" $SSH_OPTS "$SSH_USER@$SERVER_IP" \
      "jcmd $wowza_pid VM.native_memory summary 2>/dev/null | grep 'Java Heap' | awk '{
        if(\$4 ~ /committed/) {
          gsub(/[^0-9]/, \"\", \$3)
          gsub(/[^0-9]/, \"\", \$4)
          if(\$4 > 0) printf \"%.2f\", (\$3 / \$4) * 100
        }
      }'" 2>/dev/null || echo "")
  fi
  
  # Fallback 1: Try jstat if jcmd failed
  if [[ -z "$heap_raw" ]] || [[ "$heap_raw" == "0.00" ]]; then
    log "jcmd failed, trying jstat..."
    heap_raw=$(timeout 10 ssh -i "$KEY_PATH" $SSH_OPTS "$SSH_USER@$SERVER_IP" \
      "jstat -gc $wowza_pid 2>/dev/null | tail -n1 | awk '{
        eden_used=\$6; survivor_used=\$7+\$8; old_used=\$10;
        eden_max=\$1; survivor_max=\$2+\$3; old_max=\$4;
        total_used = eden_used + survivor_used + old_used;
        total_max = eden_max + survivor_max + old_max;
        if(total_max > 0) {
          printf \"%.2f\", (total_used / total_max) * 100
        } else {
          print \"0.00\"
        }
      }'" 2>/dev/null || echo "")
  fi
  
  # Fallback 2: Try jmap if both jcmd and jstat failed
  # WARNING: jmap -heap causes JVM pause - only use as emergency fallback
  # This should rarely be needed and may impact test accuracy
  if [[ -z "$heap_raw" ]] || [[ "$heap_raw" == "0.00" ]]; then
    log "WARNING: Both jcmd and jstat failed. Using jmap as last resort (may cause JVM pause)..."
    heap_raw=$(timeout 10 ssh -i "$KEY_PATH" $SSH_OPTS "$SSH_USER@$SERVER_IP" \
      "jmap -heap $wowza_pid 2>/dev/null | awk '
        /used =/ { gsub(/[^0-9]/, \"\", \$3); used=\$3 }
        /capacity =/ { gsub(/[^0-9]/, \"\", \$3); capacity=\$3 }
        END { if(capacity>0) printf \"%.2f\", (used/capacity)*100; else print \"0.00\" }
      '" 2>/dev/null || echo "0.00")
    
    if [[ "$heap_raw" != "0.00" ]]; then
      log "WARNING: jmap succeeded but may have impacted server performance during this query"
    fi
  fi
  
  echo "${heap_raw:-0.00}"
}
```

**jcmd Command Examples**:
```bash
# Method 1: GC.heap_info (RECOMMENDED - detailed, most reliable, non-intrusive)
jcmd <PID> GC.heap_info

# Output example:
# PSYoungGen      total 76288K, used 45123K
# ParOldGen       total 174592K, used 98234K
# Metaspace       used 45678K, capacity 48576K

# Method 2: VM.native_memory summary (if -XX:NativeMemoryTracking=summary enabled)
jcmd <PID> VM.native_memory summary

# NOTE: GC.heap_dump is NOT used - too slow and intrusive for live monitoring
```

#### 0.3 Implement Live Memory Monitoring Function ‚úÖ
- [x] Create `get_server_memory()` function
- [x] Query total and used memory
- [x] Calculate memory percentage
- [x] Handle SSH failures gracefully

**File**: `orchestrator/run_orchestration.sh` (lines ~545-555)
**Status**: ‚úÖ COMPLETE - Tested on production

**Implementation**:
```bash
function get_server_memory() {
  local mem_raw
  
  mem_raw=$(timeout 10 ssh -i "$KEY_PATH" $SSH_OPTS "$SSH_USER@$SERVER_IP" \
    "free | grep Mem | awk '{printf \"%.2f\", (\$3/\$2)*100}'" 2>/dev/null || echo "0.00")
  
  echo "${mem_raw:-0.00}"
}
```

#### 0.4 Implement Live Network Monitoring Function ‚úÖ
- [x] Create `get_server_network()` function
- [x] Query current network throughput
- [x] Return in Mbps
- [x] Handle multiple interfaces

**File**: `orchestrator/run_orchestration.sh` (lines ~560-565)
**Status**: ‚úÖ COMPLETE - Tested on production

**Implementation**:
```bash
function get_server_network() {
  local net_raw
  
  # Get network throughput for all interfaces (excluding lo)
  net_raw=$(timeout 10 ssh -i "$KEY_PATH" $SSH_OPTS "$SSH_USER@$SERVER_IP" \
    "cat /proc/net/dev | grep -v 'lo:' | grep ':' | awk '{rx+=\$2; tx+=\$10} END {printf \"%.2f\", (rx+tx)*8/1000000}'" 2>/dev/null || echo "0.00")
  
  echo "${net_raw:-0.00}"
}
```

#### 0.5 Create Unified Monitoring Status Function ‚úÖ
- [x] Create `check_server_status()` function
- [x] Query all metrics at once
- [x] Return structured data: `CPU|HEAP|MEMORY|NETWORK`
- [x] Add error handling for partial failures

**File**: `orchestrator/run_orchestration.sh` (lines ~567-577)
**Status**: ‚úÖ COMPLETE - Returns "CPU|HEAP|MEM|NET" format

**Implementation**:
```bash
function check_server_status() {
  local cpu=$(get_server_cpu)
  local heap=$(get_server_heap)
  local memory=$(get_server_memory)
  local network=$(get_server_network)
  
  log "Server Status - CPU: ${cpu}%, Heap: ${heap}%, Memory: ${memory}%, Network: ${network} Mbps"
  
  echo "$cpu|$heap|$memory|$network"
}
```

#### 0.6 Enhance Server-Side Monitoring Scripts ‚úÖ
- [x] Create enhanced monitoring script on server
- [x] Continuous logging of all metrics (every 5 seconds)
- [x] Timestamped entries
- [x] CSV format with all metrics

**File**: `orchestrator/remote_monitor.sh` (NEW - 235 lines)
**Status**: ‚úÖ COMPLETE
**Features**:
- Logs to CSV: `TIMESTAMP,CPU_PCT,HEAP_USED_MB,HEAP_CAPACITY_MB,HEAP_PCT,MEM_PCT,NET_MBPS,WOWZA_PID`
- Multi-GC support (Parallel, G1, ZGC, Shenandoah)
- Sudo fallback for root processes
- Enhanced stdout: `Heap: 194.00/496.00MB (39.11%)`

```bash
#!/bin/bash
# Enhanced monitoring script for server-side continuous logging

RUN_DIR="$1"
WOWZA_PID="$2"

mkdir -p "$RUN_DIR/monitors"

# Function to log with timestamp
log_metric() {
  echo "$(date -u +%Y-%m-%dT%H:%M:%SZ) $1" >> "$RUN_DIR/monitors/$2"
}

# Continuous monitoring loop
while true; do
  # CPU usage
  CPU=$(top -bn1 | grep "Cpu(s)" | awk '{print $2}' | cut -d'%' -f1)
  log_metric "$CPU" "cpu_live.log"
  
  # Memory usage
  MEM=$(free | grep Mem | awk '{printf "%.2f", ($3/$2)*100}')
  log_metric "$MEM" "memory_live.log"
  
  # Heap usage (if Wowza PID provided)
  if [[ -n "$WOWZA_PID" ]] && ps -p "$WOWZA_PID" >/dev/null 2>&1; then
    # Try jcmd first (faster, more reliable)
    HEAP=$(jcmd $WOWZA_PID GC.heap_info 2>/dev/null | awk '
      /PSYoungGen|ParOldGen|PSOldGen/ {
        if($0 ~ /total/) {
          for(i=1; i<=NF; i++) {
            if($i ~ /^[0-9]+K/) {
              gsub(/K/, "", $i)
              total_kb += $i
            }
          }
        }
        if($0 ~ /used/) {
          for(i=1; i<=NF; i++) {
            if($i ~ /^[0-9]+K/) {
              gsub(/K/, "", $i)
              used_kb += $i
            }
          }
        }
      }
      END {
        if(total_kb > 0) printf "%.2f", (used_kb / total_kb) * 100
        else print "0.00"
      }
    ' 2>/dev/null)
    
    # Fallback to jstat if jcmd fails
    if [[ -z "$HEAP" ]] || [[ "$HEAP" == "0.00" ]]; then
      HEAP=$(jstat -gc $WOWZA_PID 2>/dev/null | tail -n1 | awk '{
        used=$6+$7+$8+$10; max=$1+$2+$3+$4;
        if(max>0) printf "%.2f", (used/max)*100; else print "0.00"
      }')
    fi
    
    log_metric "${HEAP:-0.00}" "heap_live.log"
  fi
  
  # Network throughput
  NET=$(cat /proc/net/dev | grep -v 'lo:' | grep ':' | awk '{rx+=$2; tx+=$10} END {printf "%.2f", (rx+tx)*8/1000000}')
  log_metric "$NET" "network_live.log"
  
  sleep 5
done
```

#### 0.7 Create Monitoring Validation Script ‚úÖ
- [x] Create `orchestrator/validate_server.sh`
- [x] Test all monitoring functions
- [x] Verify SSH connectivity
- [x] Verify required tools on server (`jcmd`, `jstat`, `jmap`)
- [x] Test Wowza PID detection (Bootstrap Engine)
- [x] Test heap query
- [x] Generate validation report

**File**: `orchestrator/validate_server.sh` (NEW - ~150 lines)
**Status**: ‚úÖ COMPLETE - 6 validation sections

#### 0.8 Add Monitoring Health Checks ‚úÖ
- [x] Before starting tests, validate monitoring works
- [x] During tests, check if monitoring processes alive
- [x] Alert if monitoring fails mid-test
- [x] Adaptive stopping at 80% CPU or 80% Heap

**File**: `orchestrator/run_orchestration.sh` (lines ~690-720)
**Status**: ‚úÖ COMPLETE - Integrated in main orchestration loop

### Testing Phase 0 ‚úÖ

**All testing completed on production EC2 Wowza server:**

```bash
# Test 0.1: Validate monitoring tools on server ‚úÖ
./orchestrator/validate_server.sh

# Test output:
# ‚úÖ SSH connectivity: OK
# ‚úÖ CPU monitoring: OK (0.75%)
# ‚úÖ Heap monitoring: OK (100.00% - 1446MB/1446MB ZGC)
# ‚úÖ Memory monitoring: OK (28.56%)
# ‚úÖ Network monitoring: OK (0.00 Mbps)
# ‚úÖ Wowza PID detection: OK (PID: 1859 - Bootstrap Engine)
# ‚úÖ jcmd available: OK (Wowza bundled + sudo)
# ‚úÖ jstat available: OK (fallback)
# ‚úÖ jmap available: OK (emergency fallback)

# Test 0.2: Test live monitoring functions ‚úÖ
./run_orchestration.sh --pilot
# Output: Server Status: CPU=0.75% | Heap=100.00% | Mem=28.56% | Net=0.00Mbps

# Test 0.3: Test monitoring during load ‚úÖ
./run_orchestration.sh --pilot
# Verified:
# - monitor_*.log populated with CSV data
# - HEAP_USED_MB and HEAP_CAPACITY_MB columns present
# - All timestamps recent
# - No gaps in data
# - Adaptive stopping functional
```

**Success Criteria**: ‚úÖ **ALL MET**
- ‚úÖ All monitoring functions return valid percentages (0-100)
- ‚úÖ Heap monitoring works when Wowza running
- ‚úÖ Graceful fallback when tools fail (4-level cascade)
- ‚úÖ No SSH timeouts or hangs
- ‚úÖ Monitoring logs populated continuously (5-second intervals)
- ‚úÖ Validation script passes all checks
- ‚úÖ Can query metrics before, during, and after tests
- ‚úÖ Multi-GC support (Parallel, G1, ZGC, Shenandoah)
- ‚úÖ Production validated on EC2 t3.xlarge with ZGC
- ‚úÖ Adaptive stopping at 80% thresholds

**üìä Phase 0 Results Summary**:
- **Time Investment**: ~12 hours
- **Fixes Applied**: 10 major fixes
- **Functions Created**: 5 monitoring functions + 1 unified check
- **Scripts Created**: 3 (remote_monitor.sh, validate_server.sh, diagnose_jcmd.sh)
- **Documentation**: 20+ files
- **Production Testing**: ‚úÖ Validated on actual Wowza streaming server
- **Status**: ‚úÖ **PHASE 0 COMPLETE**

---

## Phase 1: Core Configuration Updates

## Phase 1: Core Configuration Updates
**Goal**: Align basic test parameters with TEST_MATRIX specification

### Tasks

#### 1.1 Update Resolution & Bitrate Mapping
- [ ] Remove multi-bitrate arrays (`BITRATES_LOW`, `BITRATES_MID`, `BITRATES_HIGH`)
- [ ] Implement single bitrate per resolution:
  - 360p: 800 kbps
  - 720p: 2,500 kbps
  - 1080p: 4,500 kbps
  - 4K: 15,000 kbps
- [ ] Store in associative array: `RESOLUTION_BITRATES`

**File**: `orchestrator/run_orchestration.sh` (lines ~230-245)

#### 1.2 Update Codec Configuration
- [ ] Lock video codec to H.264 only (remove H.265)
- [ ] Lock audio codec to AAC only (already done)
- [ ] Update variables:
  - `VIDEO_CODEC=h264` (single value, not array)
  - `AUDIO_CODEC=aac`

**File**: `orchestrator/run_orchestration.sh` (lines ~242-243)

#### 1.3 Update Connection Levels
- [ ] Change connection array to: `(1 5 10 20 50 100)`
- [ ] Remove connection level `2`
- [ ] Add connection level `100`

**File**: `orchestrator/run_orchestration.sh` (line ~244)

#### 1.4 Update Test Duration
- [ ] Change default duration to 15 minutes
- [ ] Update timing variables:
  - `DURATION_MINUTES=15`
  - Update `WARMUP`, `STEADY`, `COOLDOWN` if needed
  
**File**: `orchestrator/run_orchestration.sh` (lines ~25-35)

#### 1.5 Update Cooldown Period
- [ ] Change cooldown between tests to 30 seconds
- [ ] Update in main loop (currently 5-10 seconds)

**File**: `orchestrator/run_orchestration.sh` (lines ~506-512)

### Testing Phase 1
```bash
# Test 1: Verify configuration changes
./run_orchestration.sh --pilot

# Expected:
# - Single bitrate per resolution
# - H.264/AAC only
# - Connection levels: 1, 5, 10, 20, 50, 100
# - 15-minute duration (or pilot mode 2 min)
# - 30-second cooldown between tests
```

**Success Criteria**:
- ‚úÖ Pilot mode runs with new parameters
- ‚úÖ Logs show correct bitrates
- ‚úÖ No multi-codec tests
- ‚úÖ Connection count includes 100
- ‚úÖ Cooldown period is 30 seconds

---

## Phase 2: Test Execution Order Restructuring
**Goal**: Change from protocol-first to resolution-first ordering

### Tasks

#### 2.1 Reorder Test Loops
- [ ] Restructure main test loop to:
  ```bash
  for resolution in "${RESOLUTIONS[@]}"; do
    for protocol in "${PROTOCOLS[@]}"; do
      for conn in "${CONNECTIONS[@]}"; do
        # Test execution
      done
    done
  done
  ```
- [ ] Update resolution order to: `(360p 720p 1080p 4k)` (lowest to highest)
- [ ] Remove video codec loop (single codec now)
- [ ] Remove bitrate loop (single bitrate per resolution)

**File**: `orchestrator/run_orchestration.sh` (lines ~470-520)

#### 2.2 Update Logging
- [ ] Update log messages to reflect new order
- [ ] Log format: `"Starting Resolution: ${resolution}"`
- [ ] Log format: `"  Protocol: ${protocol}, Connections: ${conn}"`

### Testing Phase 2
```bash
# Test 2: Verify execution order
./run_orchestration.sh --pilot

# Expected order:
# Test 1: RTMP, 360p, 1 connection
# Test 2: RTMP, 360p, 5 connections
# ...
# Test 6: RTMP, 360p, 100 connections
# Test 7: RTSP, 360p, 1 connection
# ...
# Test 18: SRT, 360p, 100 connections
# Test 19: RTMP, 720p, 1 connection
```

**Success Criteria**:
- ‚úÖ Tests execute in resolution-first order
- ‚úÖ All protocols tested for each resolution
- ‚úÖ Connection count scales properly
- ‚úÖ Logs clearly show test progression

---

## Phase 3: Heap Memory Monitoring Implementation
**Goal**: Add Java heap memory monitoring alongside CPU monitoring

### Tasks

#### 3.1 Create Heap Monitoring Function
- [ ] Create `get_server_heap()` function
- [ ] Use `jstat` or JMX to query Java heap usage
- [ ] Return percentage: `(used_heap / max_heap) * 100`
- [ ] Handle cases where Wowza PID not detected
- [ ] Add fallback to `jmap` if `jstat` unavailable

**File**: `orchestrator/run_orchestration.sh` (new function after `get_server_cpu()`)

**Implementation**:
```bash
function get_server_heap() {
  local heap_raw
  local wowza_pid
  
  # Get Wowza PID
  wowza_pid=$(ssh -i "$KEY_PATH" $SSH_OPTS "$SSH_USER@$SERVER_IP" \
    "ps aux | grep -E '[Ww]owza|java.*com.wowza' | grep -v grep | head -n1 | awk '{print \$2}'" 2>/dev/null || echo "")
  
  if [[ -z "$wowza_pid" ]]; then
    log "WARNING: Could not detect Wowza PID for heap monitoring"
    echo "0.00"
    return
  fi
  
  # Use jstat to get heap usage
  heap_raw=$(ssh -i "$KEY_PATH" $SSH_OPTS "$SSH_USER@$SERVER_IP" \
    "jstat -gc $wowza_pid | tail -n1 | awk '{used=\$3+\$4+\$6+\$8; max=\$1+\$2; if(max>0) print (used/max)*100; else print 0}'" 2>/dev/null || echo "0.00")
  
  echo "$heap_raw"
}
```

#### 3.2 Enhance Remote Monitoring
- [ ] Verify `jstat` monitoring is captured during tests
- [ ] Ensure heap metrics included in `jstat_gc.log`
- [ ] Add heap summary to `wowza_proc.txt`

**File**: `orchestrator/run_orchestration.sh` (`remote_start_monitors()` function)

#### 3.3 Update Parser for Heap Metrics
- [ ] Modify `parse_run.py` to extract heap data from `jstat_gc.log`
- [ ] Add heap columns to CSV output:
  - `heap_used_mb`
  - `heap_max_mb`
  - `heap_percent`
  - `peak_heap_percent`

**File**: `orchestrator/parse_run.py`

### Testing Phase 3
```bash
# Test 3: Verify heap monitoring
./run_orchestration.sh --pilot

# Manual checks:
# 1. Check orchestrator.log for heap percentage readings
# 2. Verify jstat_gc.log contains heap data
# 3. Check results.csv for heap columns
# 4. Confirm heap values are reasonable (0-100%)
```

**Success Criteria**:
- ‚úÖ Heap percentage logged before each test
- ‚úÖ `jstat_gc.log` captured for each run
- ‚úÖ CSV contains heap metrics
- ‚úÖ Heap values are within 0-100% range
- ‚úÖ Graceful handling when PID not found

---

## Phase 4: Adaptive Threshold Stopping Logic
**Goal**: Implement intelligent stopping when CPU ‚â• 80% OR Heap ‚â• 80%

### Tasks

#### 4.1 Implement Dual-Threshold Checking
- [ ] Create `check_server_thresholds()` function
- [ ] Check both CPU and Heap before each test
- [ ] Return status: `PASS`, `CPU_LIMIT`, `HEAP_LIMIT`, or `BOTH_LIMIT`
- [ ] Log detailed threshold status

**File**: `orchestrator/run_orchestration.sh` (new function)

**Implementation**:
```bash
function check_server_thresholds() {
  local cpu=$(get_server_cpu)
  local heap=$(get_server_heap)
  
  cpu=${cpu:-0}
  heap=${heap:-0}
  
  local cpu_int=${cpu%.*}
  local heap_int=${heap%.*}
  
  log "Server thresholds - CPU: ${cpu}%, Heap: ${heap}%"
  
  if (( cpu_int >= 80 && heap_int >= 80 )); then
    echo "BOTH_LIMIT|$cpu|$heap"
  elif (( cpu_int >= 80 )); then
    echo "CPU_LIMIT|$cpu|$heap"
  elif (( heap_int >= 80 )); then
    echo "HEAP_LIMIT|$cpu|$heap"
  else
    echo "PASS|$cpu|$heap"
  fi
}
```

#### 4.2 Implement Skip Logic
- [ ] When threshold reached, skip remaining connection levels
- [ ] Continue to next protocol
- [ ] Continue to next resolution if all protocols done
- [ ] Track "maximum capacity" for each protocol/resolution

**File**: `orchestrator/run_orchestration.sh` (main loop)

#### 4.3 Add State Tracking
- [ ] Create associative array to track max capacity:
  ```bash
  declare -A MAX_CAPACITY
  # Format: MAX_CAPACITY[protocol_resolution]="connections|cpu|heap"
  ```
- [ ] Store last successful test before threshold
- [ ] Use for summary report

#### 4.4 Update Main Loop Logic
- [ ] Before each test, call `check_server_thresholds()`
- [ ] If limit reached:
  - Log maximum capacity
  - Store in `MAX_CAPACITY` array
  - Break from connection loop
  - Continue to next protocol
- [ ] Add flags to track if we're skipping

**File**: `orchestrator/run_orchestration.sh` (lines ~470-520)

### Testing Phase 4
```bash
# Test 4a: Verify threshold detection
# Manually set low threshold for testing (e.g., 40%)
./run_orchestration.sh --pilot

# Test 4b: Verify skip logic
# Watch logs to ensure:
# - Tests stop when threshold reached
# - Remaining connections skipped
# - Next protocol/resolution starts
```

**Success Criteria**:
- ‚úÖ CPU ‚â• 80% stops tests
- ‚úÖ Heap ‚â• 80% stops tests
- ‚úÖ Either threshold stops tests
- ‚úÖ Skipped tests logged clearly
- ‚úÖ Maximum capacity recorded
- ‚úÖ Next protocol/resolution continues

---

## Phase 5: Enhanced Logging & Reporting
**Goal**: Implement comprehensive logging with maximum capacity tracking

### Tasks

#### 5.1 Enhance Per-Test Logging
- [ ] Create detailed test log format per TEST_MATRIX spec
- [ ] Include all parameters:
  - Test number
  - Timestamp (start/end)
  - Protocol, Resolution, Connections
  - Bitrates (video/audio)
  - Duration (actual)
  - Server URL
  - Stream names
- [ ] Add server metrics section:
  - Initial CPU/Heap
  - Peak CPU/Heap
  - Final CPU/Heap
  - Peak bandwidth
- [ ] Add result status:
  - `SUCCESS` - completed normally
  - `STOPPED` - threshold reached
  - `FAILED` - error occurred

**File**: `orchestrator/run_orchestration.sh` (`run_single_experiment()` function)

#### 5.2 Create Test Results Structure
- [ ] Create per-test results file: `$run_dir/test_result.json`
- [ ] Store structured data for easy parsing
- [ ] Include threshold status if stopped

**File**: `orchestrator/run_orchestration.sh`

#### 5.3 Implement Summary Report Generator
- [ ] Create `generate_summary_report()` function
- [ ] Called at end of test sweep
- [ ] Generate report per TEST_MATRIX format
- [ ] Include:
  - Total tests run
  - Total duration
  - Maximum capacities by protocol/resolution
  - Peak CPU/Heap for each
  - Total bandwidth achieved

**File**: `orchestrator/run_orchestration.sh` (new function, called at end)

**Report Format**:
```
=== Load Test Summary Report ===
Date: [timestamp]
Server IP: [IP]
Total Tests Run: [X/72]
Total Duration: [HH:MM:SS]

Maximum Capacities:
RTMP:
  360p: 100 connections (CPU: 72%, Heap: 65%)
  720p: 50 connections (CPU: 78%, Heap: 71%)
  1080p: 20 connections (CPU: 81%, Heap: 68%) [STOPPED: CPU_LIMIT]
  4K: 5 connections (CPU: 75%, Heap: 82%) [STOPPED: HEAP_LIMIT]

RTSP:
  [... similar ...]

SRT:
  [... similar ...]

Total Bandwidth Achieved:
  RTMP: 450 Mbps
  RTSP: 380 Mbps
  SRT: 420 Mbps
```

#### 5.4 Add Progress Tracking
- [ ] Show test count: "Test 23/72"
- [ ] Show estimated time remaining
- [ ] Show current phase: "Resolution: 720p, Protocol: RTSP"

**File**: `orchestrator/run_orchestration.sh` (main loop)

### Testing Phase 5
```bash
# Test 5: Verify logging and reporting
./run_orchestration.sh --pilot

# Verify:
# 1. Per-test logs contain all required fields
# 2. test_result.json files created
# 3. Summary report generated
# 4. Progress tracking shows correctly
# 5. Maximum capacities recorded accurately
```

**Success Criteria**:
- ‚úÖ Detailed per-test logs generated
- ‚úÖ JSON result files created
- ‚úÖ Summary report generated at end
- ‚úÖ Report shows max capacities
- ‚úÖ Progress tracking displays correctly
- ‚úÖ All metrics included and accurate

---

## Phase 6: User Input & Configuration
**Goal**: Streamline user inputs for test matrix execution

### Tasks

#### 6.1 Update Input Prompts
- [ ] Simplify prompts for test matrix mode
- [ ] Remove codec selection (fixed to H.264/AAC)
- [ ] Remove bitrate selection (per-resolution)
- [ ] Add confirmation for full 72-test run

**File**: `orchestrator/run_orchestration.sh` (input section)

#### 6.2 Add Test Matrix Mode Flag
- [ ] Add `--test-matrix` flag
- [ ] Automatically use TEST_MATRIX configuration
- [ ] Skip interactive prompts where possible

**File**: `orchestrator/run_orchestration.sh` (argument parsing)

#### 6.3 Create Configuration Validation
- [ ] Validate server endpoints are reachable
- [ ] Verify required monitoring tools installed
- [ ] Check client resources (CPU, network)
- [ ] Warn if network bandwidth insufficient

**File**: New script `orchestrator/validate_client.sh`

#### 6.4 Update Pilot Mode
- [ ] Adjust pilot mode for TEST_MATRIX
- [ ] Use: 1 resolution, 1 protocol, 3 connection levels
- [ ] Shorter duration (2 minutes)
- [ ] Test adaptive stopping with low threshold

**File**: `orchestrator/run_orchestration.sh` (pilot mode section)

### Testing Phase 6
```bash
# Test 6a: Test matrix mode
./run_orchestration.sh --test-matrix --pilot

# Test 6b: Validation
./orchestrator/validate_client.sh

# Test 6c: Full prompts
./run_orchestration.sh
```

**Success Criteria**:
- ‚úÖ Test matrix mode works without manual inputs
- ‚úÖ Validation catches issues early
- ‚úÖ Pilot mode tests adaptive features
- ‚úÖ User prompts clear and minimal
- ‚úÖ Configuration errors caught before tests start

---

## Phase 7: Error Handling & Resilience
**Goal**: Ensure robust operation with proper error handling

### Tasks

#### 7.1 Enhance Connection Error Handling
- [ ] Retry failed SSH connections (already partially done)
- [ ] Retry failed stream connections
- [ ] Log connection failures separately
- [ ] Don't count failed tests toward threshold

**File**: `orchestrator/run_orchestration.sh` (multiple functions)

#### 7.2 Add Test Recovery
- [ ] Save state before each test
- [ ] Allow resuming from last successful test
- [ ] Create checkpoint file: `runs/checkpoint.json`

**File**: `orchestrator/run_orchestration.sh` (new checkpoint functions)

#### 7.3 Improve Interrupt Handling
- [ ] Save results on Ctrl+C
- [ ] Generate partial summary report
- [ ] Clean up remote processes
- [ ] Mark incomplete tests clearly

**File**: `orchestrator/run_orchestration.sh` (`on_interrupt()` function)

#### 7.4 Add Sanity Checks
- [ ] Verify FFmpeg processes actually started
- [ ] Check stream connections established
- [ ] Monitor for dropped streams during test
- [ ] Validate monitoring data collected

**File**: `orchestrator/run_orchestration.sh` (`run_single_experiment()` function)

### Testing Phase 7
```bash
# Test 7a: Connection failures
# Disconnect network mid-test, verify recovery

# Test 7b: Interrupt handling
# Press Ctrl+C during test, verify cleanup

# Test 7c: Resume functionality
# Stop tests, then resume from checkpoint
```

**Success Criteria**:
- ‚úÖ Failed connections don't crash orchestrator
- ‚úÖ Tests resume after interruption
- ‚úÖ Checkpoint file saves/loads correctly
- ‚úÖ Cleanup happens on all exit paths
- ‚úÖ Partial results saved on interrupt

---

## Phase 8: Documentation & Polish
**Goal**: Complete documentation and user experience improvements

### Tasks

#### 8.1 Update Documentation
- [ ] Update `orchestrator/docs/README.md` with new features
- [ ] Create usage examples for TEST_MATRIX mode
- [ ] Document all command-line flags
- [ ] Add troubleshooting section

**Files**: `orchestrator/docs/README.md`, `orchestrator/docs/USAGE.md`

#### 8.2 Add Help Text
- [ ] Implement `--help` flag
- [ ] Show available options
- [ ] Explain test matrix mode
- [ ] Include examples

**File**: `orchestrator/run_orchestration.sh` (new help function)

#### 8.3 Improve Output Formatting
- [ ] Add color-coded output (success/warning/error)
- [ ] Add progress bars for long tests
- [ ] Pretty-print summary report
- [ ] Add ASCII table formatting

**File**: `orchestrator/run_orchestration.sh` (output functions)

#### 8.4 Create Quick Start Guide
- [ ] Create `QUICKSTART.md` in orchestrator/docs
- [ ] Include minimal example
- [ ] Show pilot mode usage
- [ ] Link to full TEST_MATRIX documentation

**File**: `orchestrator/docs/QUICKSTART.md` (new)

### Testing Phase 8
```bash
# Test 8: User experience
./run_orchestration.sh --help
./run_orchestration.sh --test-matrix --pilot

# Verify:
# - Help text is clear
# - Output is readable
# - Documentation is complete
# - Examples work as documented
```

**Success Criteria**:
- ‚úÖ Help text displays correctly
- ‚úÖ Documentation complete and accurate
- ‚úÖ Examples work as shown
- ‚úÖ Output is clear and well-formatted
- ‚úÖ Quick start guide works for new users

---

## Phase 9: Integration Testing & Validation
**Goal**: End-to-end testing of complete test matrix

### Tasks

#### 9.1 Full Test Matrix Dry Run
- [ ] Run complete 72-test matrix in pilot mode (2 min tests)
- [ ] Verify all 72 tests execute or skip correctly
- [ ] Check adaptive stopping works
- [ ] Verify summary report accuracy

#### 9.2 Threshold Testing
- [ ] Test with real server reaching 80% CPU
- [ ] Test with real server reaching 80% Heap
- [ ] Verify correct protocol/resolution skipping
- [ ] Verify maximum capacity recorded

#### 9.3 Long-Duration Testing
- [ ] Run subset with full 15-minute tests
- [ ] Verify monitoring stays active entire time
- [ ] Check log file sizes are manageable
- [ ] Ensure no memory leaks in orchestrator

#### 9.4 Multi-Server Testing
- [ ] Test against different server types
- [ ] Verify compatibility with Wowza, SRS, MediaMTX
- [ ] Check heap monitoring works (or gracefully skips)

#### 9.5 Performance Validation
- [ ] Measure orchestrator overhead
- [ ] Verify network bandwidth calculations
- [ ] Check CPU usage on client machine
- [ ] Validate timing accuracy (15 min = 15 min)

### Testing Phase 9
```bash
# Test 9a: Full dry run
./run_orchestration.sh --test-matrix --pilot

# Test 9b: Real threshold test
./run_orchestration.sh --test-matrix
# Let it reach actual thresholds

# Test 9c: Long duration
./run_orchestration.sh --test-matrix --resolutions 360p --protocols rtmp

# Test 9d: Multi-server
./run_orchestration.sh --test-matrix --pilot --server-ip [different-server]
```

**Success Criteria**:
- ‚úÖ All 72 tests execute correctly (or stop appropriately)
- ‚úÖ Adaptive stopping works in real scenarios
- ‚úÖ Long tests complete without issues
- ‚úÖ Works with multiple server types
- ‚úÖ Performance is acceptable
- ‚úÖ Timing is accurate
- ‚úÖ No crashes or hangs

---

## Phase 10: Production Readiness
**Goal**: Final polish and production deployment

### Tasks

#### 10.1 Code Review & Cleanup
- [ ] Review all code changes
- [ ] Remove debug statements
- [ ] Clean up commented code
- [ ] Optimize performance bottlenecks
- [ ] Add code comments

#### 10.2 Security Review
- [ ] Audit SSH key handling
- [ ] Check file permissions
- [ ] Validate input sanitization
- [ ] Review remote command injection risks

#### 10.3 Create Release Notes
- [ ] Document all new features
- [ ] List breaking changes
- [ ] Include migration guide from old version
- [ ] Add known limitations

**File**: `orchestrator/docs/RELEASE_NOTES.md` (new)

#### 10.4 Version Tagging
- [ ] Update version number in scripts
- [ ] Tag git commit: `v2.0.0-test-matrix`
- [ ] Create GitHub release
- [ ] Update main README.md

### Testing Phase 10
```bash
# Final validation
# Run complete test suite
./run_orchestration.sh --test-matrix

# Verify:
# - No errors or warnings
# - Complete summary report
# - All logs captured
# - Results accurate
```

**Success Criteria**:
- ‚úÖ Code review complete
- ‚úÖ Security issues addressed
- ‚úÖ Documentation complete
- ‚úÖ Version tagged
- ‚úÖ Production-ready

---

## Implementation Timeline Estimate

| Phase | Description | Estimated Time | Complexity | Priority |
|-------|-------------|----------------|------------|----------|
| **Phase 0** | **Monitoring Infrastructure** | **6-8 hours** | **High** | **üî• CRITICAL** |
| Phase 1 | Core Configuration | 2-3 hours | Low | High |
| Phase 2 | Test Order Restructuring | 2-3 hours | Low | Medium |
| Phase 3 | Enhanced Heap Parsing | 2-3 hours | Low | Medium |
| Phase 4 | Adaptive Stopping | 6-8 hours | High | üî• CRITICAL |
| Phase 5 | Logging & Reporting | 4-6 hours | Medium | High |
| Phase 6 | User Input & Config | 3-4 hours | Low | Medium |
| Phase 7 | Error Handling | 4-6 hours | Medium | High |
| Phase 8 | Documentation | 3-4 hours | Low | Medium |
| Phase 9 | Integration Testing | 8-12 hours | High | üî• CRITICAL |
| Phase 10 | Production Ready | 4-6 hours | Medium | High |
| **TOTAL** | **All Phases** | **46-66 hours** | **6-8 days** | - |

### ‚ö†Ô∏è Phase 0 is Now a Prerequisite

**Phase 0 must be completed before all other phases** because:
- Adaptive stopping (Phase 4) requires live heap monitoring
- Configuration changes (Phase 1) need validation
- Test order changes (Phase 2) need working metrics
- All phases depend on reliable server monitoring

**Recommended Order**:
1. **Phase 0** - Monitoring infrastructure ‚Üê START HERE
2. **Phase 4** - Adaptive stopping (uses Phase 0 functions)
3. **Phase 1** - Core configuration
4. **Phase 2** - Test order
5. **Phase 5** - Logging/reports
6. **Phases 6-10** - Polish and finalize

---

## Risk Mitigation

### High-Risk Areas

1. **Live Monitoring Reliability** üî•
   - Risk: SSH connection drops during metric queries
   - Mitigation: Timeout protection, retry logic, fallback to cached values
   - Testing: Simulate network issues, verify graceful degradation

2. **Heap Monitoring Accuracy**
   - Risk: `jstat` output varies by Java version, Wowza PID detection fails
   - Mitigation: Test with multiple Java versions, multiple fallback methods (`jstat` ‚Üí `jmap` ‚Üí `ps`)
   - Testing: Test on Java 8, 11, 17; test without Wowza running

3. **Server-Side Logging Gaps**
   - Risk: Monitoring processes die mid-test, logs incomplete
   - Mitigation: Health checks, auto-restart monitoring, alert on gaps
   - Testing: Kill monitoring processes mid-test, verify detection/restart

4. **Adaptive Stopping Logic**
   - Risk: Incorrect threshold detection causes premature stopping or over-testing
   - Mitigation: Conservative thresholds (80%), multiple metric confirmation, extensive testing
   - Testing: Test with various threshold values (60%, 70%, 80%, 90%)

5. **Long Test Duration**
   - Risk: 18-hour tests may fail mid-way, monitoring data loss
   - Mitigation: Implement checkpointing, continuous log writes, resume capability
   - Testing: Run overnight tests, simulate failures at various points

6. **Network Reliability**
   - Risk: SSH connections drop during tests, can't query metrics
   - Mitigation: Connection keepalive, retry logic, local caching
   - Testing: Disconnect/reconnect network, verify recovery

### Dependencies

**Server-Side (4-core/8GB EC2)**:
- ‚úÖ Java/JVM (for Wowza)
- üî• `jcmd` utility (for heap monitoring) - **PRIMARY METHOD** - **CRITICAL**
- üî• `jstat` utility (fallback for heap) - **CRITICAL**
- üî• `jmap` utility (fallback #2 for heap) - **CRITICAL**
- ‚úÖ `pidstat` (from sysstat package)
- ‚úÖ `sar` (from sysstat package)
- ‚ö†Ô∏è `ifstat` (optional, for detailed network stats)
- ‚úÖ SSH server with key-based auth
- ‚úÖ Bash shell

**Note on Java Tools**:
- `jcmd` - **Preferred** (modern, reliable, easy to parse)
- `jstat` - Fallback (widely available, older tool)
- `jmap` - Second fallback (last resort)
- All three come with standard JDK installation

**Client-Side (8-core/16GB EC2)**:
- ‚úÖ Python 3 (for result parsing)
- ‚úÖ FFmpeg with H.264/AAC support
- ‚úÖ SSH client
- ‚úÖ Bash shell
- ‚úÖ Network connectivity to server

**Installation Commands** (for server):
```bash
# Ubuntu/Debian
sudo apt-get update
sudo apt-get install -y sysstat ifstat openjdk-11-jdk-headless

# Verify jcmd available (preferred)
which jcmd
jcmd -h

# Verify jstat available (fallback)
which jstat
jstat -help

# Verify jmap available (fallback #2)
which jmap
jmap -help

# Test jcmd with running Java process
jcmd <PID> GC.heap_info
```

**Why jcmd is Better**:
```
jcmd vs jstat comparison:

jcmd GC.heap_info:
  ‚úÖ Human-readable output
  ‚úÖ Clear "used" and "total" values
  ‚úÖ Easier to parse with awk
  ‚úÖ More reliable across Java versions
  ‚úÖ Modern recommended tool
  ‚úÖ Single command for heap summary

jstat -gc:
  ‚ö†Ô∏è Column-based output (harder to parse)
  ‚ö†Ô∏è Requires manual calculation
  ‚ö†Ô∏è Output format varies by Java version
  ‚ö†Ô∏è Older tool
  ‚úÖ More widely known
  ‚úÖ Still reliable

Example jcmd output:
  PSYoungGen      total 76288K, used 45123K
  ParOldGen       total 174592K, used 98234K
  ‚Üí Easy to extract: 143357K used / 250880K total = 57.14%

Example jstat output:
  S0C    S1C    S0U    S1U      EC       EU        OC         OU
  1024.0 1024.0  0.0  512.0  76288.0  45123.0  174592.0   98234.0
  ‚Üí Harder to parse, need to know which columns to sum
```

---

## Phase 0 Detailed Breakdown

### Why Phase 0 is Critical

The entire test matrix approach depends on **accurate, real-time monitoring** of the server:

```
Without Phase 0:                    With Phase 0:
‚îú‚îÄ Can't detect 80% threshold   ‚îÄ‚îÄ‚ñ∫ ‚úÖ Real-time CPU/Heap monitoring
‚îú‚îÄ Can't stop tests safely      ‚îÄ‚îÄ‚ñ∫ ‚úÖ Adaptive stopping works
‚îú‚îÄ Risk server overload         ‚îÄ‚îÄ‚ñ∫ ‚úÖ Server protected
‚îú‚îÄ No capacity data             ‚îÄ‚îÄ‚ñ∫ ‚úÖ Maximum capacity logged
‚îî‚îÄ Wasted test time             ‚îÄ‚îÄ‚ñ∫ ‚úÖ Time-efficient testing
```

### Current Monitoring vs. Phase 0 Enhanced

| Metric | Currently | Phase 0 Enhancement | Why Critical |
|--------|-----------|---------------------|--------------|
| **CPU** | ‚úÖ `get_server_cpu()` exists | ‚úÖ Already works | For adaptive stopping |
| **Heap** | ‚ö†Ô∏è Logged to file only | üî• Add `get_server_heap()` | **PRIMARY SERVER BOTTLENECK** |
| **Memory** | ‚ö†Ô∏è Not queried live | üîß Add `get_server_memory()` | Capacity planning |
| **Network** | ‚ö†Ô∏è Not queried live | üîß Add `get_server_network()` | Bandwidth validation |
| **Validation** | ‚ùå None | üî• Add pre-test checks | **PREVENTS FAILED RUNS** |
| **Continuous** | ‚ö†Ô∏è pidstat logs only | üîß Enhanced monitoring | Complete time-series data |

### What Phase 0 Delivers

1. **Live Query Functions** (callable anytime during tests)
   ```bash
   get_server_cpu    # Returns: 45.2
   get_server_heap   # Returns: 67.8
   get_server_memory # Returns: 52.3
   get_server_network # Returns: 245.6 (Mbps)
   ```

2. **Validation Script** (run before tests)
   ```bash
   ./orchestrator/validate_monitoring.sh
   # Checks all monitoring works
   # Verifies server tools installed
   # Tests Wowza PID detection
   # Confirms heap queries work
   ```

3. **Continuous Logging** (server-side)
   ```bash
   monitors/cpu_live.log      # CPU every 5 seconds
   monitors/heap_live.log     # Heap every 5 seconds
   monitors/memory_live.log   # Memory every 5 seconds
   monitors/network_live.log  # Network every 5 seconds
   ```

4. **Health Checks** (during tests)
   ```bash
   # Before each test:
   - Verify monitoring processes alive
   - Check log files updating
   - Alert if monitoring fails
   - Attempt restart if needed
   ```

### Phase 0 Testing Strategy

```bash
# Step 1: Install server dependencies
ssh ubuntu@[SERVER_IP]
sudo apt-get install -y sysstat ifstat openjdk-11-jdk-headless

# Step 2: Start Wowza
# [Start Wowza on server]

# Step 3: Run validation script
./orchestrator/validate_monitoring.sh \
  --server-ip [SERVER_IP] \
  --ssh-key ~/.ssh/key.pem \
  --ssh-user ubuntu

# Expected output:
# ‚úÖ SSH connectivity: OK
# ‚úÖ Wowza PID: 12345
# ‚úÖ CPU monitoring: OK (12.3%)
# ‚úÖ Heap monitoring: OK (34.5%)
# ‚úÖ Memory monitoring: OK (45.6%)
# ‚úÖ Network monitoring: OK (0.5 Mbps)
# ‚úÖ Required tools: jstat, jmap, pidstat, sar, ifstat
# ‚úÖ All checks passed!

# Step 4: Test live queries manually
./run_orchestration.sh
# [Enter server details]
# [Abort before starting tests]
# Check logs for successful metric queries

# Step 5: Run pilot test
./run_orchestration.sh --pilot
# Verify continuous logging works
# Check all log files populated
# Verify no gaps in data
```

---

## Testing Strategy

### Unit Testing
- Test each function independently
- Mock SSH connections for local testing
- Validate threshold calculations

### Integration Testing
- Test complete workflows
- Verify cross-function interactions
- Check data flow from monitoring to reports

### System Testing
- Run full 72-test matrix
- Test on multiple server types
- Verify with real load scenarios

### Regression Testing
- Ensure pilot mode still works
- Verify backward compatibility
- Test with existing run data

---

## Success Metrics

### Functional Metrics
- ‚úÖ All 72 tests can execute
- ‚úÖ Adaptive stopping works reliably
- ‚úÖ Maximum capacity tracked accurately
- ‚úÖ Summary report generated correctly

### Performance Metrics
- ‚úÖ Orchestrator overhead < 5% CPU
- ‚úÖ Memory usage stable over 18 hours
- ‚úÖ No significant drift in timing
- ‚úÖ Log files manageable size

### Usability Metrics
- ‚úÖ Clear progress indication
- ‚úÖ Easy to understand reports
- ‚úÖ Minimal user input required
- ‚úÖ Good error messages

---

## Rollout Plan

### Stage 1: Development (Phases 1-8)
- Implement features incrementally
- Test each phase before proceeding
- Commit code after each successful phase

### Stage 2: Testing (Phase 9)
- Comprehensive integration testing
- Multi-environment validation
- Performance benchmarking

### Stage 3: Beta Release (Phase 10)
- Limited release to test users
- Gather feedback
- Fix critical issues

### Stage 4: Production Release
- Full documentation
- Tagged release
- Announcement and training

---

## Next Steps

1. **Review this plan** - Ensure alignment with requirements
2. **Start Phase 1** - Begin with configuration updates
3. **Set up test environment** - Prepare test servers
4. **Create tracking** - Use GitHub issues or similar for task tracking
5. **Begin development** - Start implementing phases sequentially

---

**Document Version**: 1.0  
**Created**: October 17, 2025  
**Last Updated**: October 17, 2025  
**Status**: Ready for Implementation
